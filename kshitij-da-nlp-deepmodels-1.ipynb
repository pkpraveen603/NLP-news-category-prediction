{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/ktjqdataanalysis\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c113dfa233512b35979de2de81b3465e40a6410"},"cell_type":"markdown","source":"**IMPORTING LIBRARIES TO BE USED**"},{"metadata":{"trusted":true,"_uuid":"e36a977e69d08735c0ca0f2752e5368d9cdf8f35"},"cell_type":"code","source":"#Importing Libraries to be used\n\nimport random\nimport keras\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom scipy.interpolate import spline\nfrom keras.models import load_model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import backend as K\nfrom keras.models import Model\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.layers import GlobalMaxPool1D, Dropout\nfrom keras.layers import TimeDistributed\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, SpatialDropout1D, Bidirectional, Embedding,SimpleRNN, Dense, Flatten, Input, Conv1D, MaxPooling1D, CuDNNGRU, concatenate, CuDNNLSTM\nfrom keras.layers import AveragePooling1D, BatchNormalization\nfrom sklearn.metrics import f1_score\nfrom keras.engine.topology import Layer\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom tqdm import tqdm\ntqdm.pandas()\nimport matplotlib.pyplot as plt\nimport gc\ngc.enable()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da31b994de3fe2949a52370a66a2834d11a62d12"},"cell_type":"markdown","source":"**READING IN GIVEN DATA**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"given_data = pd.read_csv('../input/ktjqdataanalysis/dataset_convertedTocsv.csv')\ntest_data = pd.read_json('../input/test-set/test_set.json')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a76282ce3f8dabe06176512ea9622738dbe827a6"},"cell_type":"markdown","source":"**CONVERTING EVERY ROW TO STRING, AND INTO LOWERCASE (SOME EMBEDDINGS DON'T UNDERSTAND CAPS)**"},{"metadata":{"trusted":true,"_uuid":"b6e7feae8a9acb33e7fb49a92c98343a644265f5"},"cell_type":"code","source":"given_data['headline']=given_data['headline'].progress_apply(lambda x: str(x).lower())\ngiven_data['short_description']=given_data['short_description'].progress_apply(lambda x: str(x).lower())\n\ntest_data['headline']=test_data['headline'].progress_apply(lambda x: str(x).lower())\ntest_data['short_description']=test_data['short_description'].progress_apply(lambda x: str(x).lower())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11edb1dc671503e26849de76cf4dc5746ae89708"},"cell_type":"markdown","source":"**REMOVING PUNCTUATIONS**"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"ccd650caecdea7a169fec5a9a99816387862da5a"},"cell_type":"code","source":"#Removing Punctuations\n\npunct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\npunct = \"/-'!#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n\ndef clean_special_chars(text, punct, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    \n    for p in punct:\n        text = text.replace(p, f' ')\n    \n    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters that I have to deal with in last\n    for s in specials:\n        text = text.replace(s, specials[s])\n    \n    return text\n\n\ngiven_data['headline'] = given_data['headline'].progress_apply(lambda x: clean_special_chars(x, punct, punct_mapping))\ngiven_data['short_description'] = given_data['short_description'].progress_apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n\ntest_data['headline'] = test_data['headline'].progress_apply(lambda x: clean_special_chars(x, punct, punct_mapping))\ntest_data['short_description'] = test_data['short_description'].progress_apply(lambda x: clean_special_chars(x, punct, punct_mapping))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ef152b3b4018c350c0255eee67ab06fbf497ffd"},"cell_type":"code","source":"#CONVERTING CATEGORIES INTO NUMERICAL ID\n\ngiven_data['category_id']=given_data['category'].factorize()[0]\ncategory_id_df = given_data[['category', 'category_id']].drop_duplicates().sort_values('category_id')\ncategory_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[['category_id', 'category']].values)\ngiven_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43fa891f7b901d31133a0c2523e9e1b2c433ab60"},"cell_type":"code","source":"#MERGING HEADLINES AND SHORT DESCRIPTIONS INTO A SINGLE COL IN GIVEN_DATA\n\nmerged=[]\nfor row in range(len(given_data)):\n    temp_word=''\n    headline=str(given_data.headline[row])\n    shortDescription=str(given_data.short_description[row])\n    temp_information=headline + ' ' + shortDescription\n    merged.append(temp_information)\n\ngiven_data['merged_headline_shortDescription']=merged\n\nmerged=[]\nfor row in range(len(test_data)):\n    temp_word=''\n    headline=str(test_data.headline[row])\n    shortDescription=str(test_data.short_description[row])\n    temp_information=headline + ' ' + shortDescription\n    merged.append(temp_information)\n    \ntest_data['merged_headline_shortDescription']=merged","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9afd0e3200c1a6cf86dfc60b8a2f6e2f0953db6d"},"cell_type":"code","source":"#REMOVING NUMBERS, SYMBOLS AND STOPWORDS. REDUCING EVERYTHING TO ITS \"LEMMA\" FORM\n\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nlemmetizer = WordNetLemmatizer()\nstemmer = PorterStemmer()\ndef get_words(text):\n    text_only_letters = re.sub('[^a-zA-Z]', ' ', text)\n    words = nltk.word_tokenize(text_only_letters.lower())\n    stops = set(stopwords.words('english'))\n    meaningful_words = [lemmetizer.lemmatize(w) for w in words if w not in stops]\n    return ' '.join(meaningful_words)\n\ngiven_data['merged_headline_shortDescription'] = given_data['merged_headline_shortDescription'].progress_apply(lambda x: get_words(x))\ntest_data['merged_headline_shortDescription'] = test_data['merged_headline_shortDescription'].progress_apply(lambda x: get_words(x))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f995ad5ad3db8a02d4897bc6dc8f6cebf29b644"},"cell_type":"markdown","source":"**STARTING TRYING OUT DEEP MODELS**"},{"metadata":{"_uuid":"476ec134b08df401b68717f86d10adfd2a6f34bd"},"cell_type":"markdown","source":"**CONVERTION TO ARRAYS, TOKENIZING & PADDING**"},{"metadata":{"trusted":true,"_uuid":"3db1687e27243fa645f7b532281f0ec7d43932a2","scrolled":true},"cell_type":"code","source":"labels = given_data['category_id']\nmax_len = 20\nmax_words = 100000\ntokenizer = Tokenizer(num_words = max_words)\ntokenizer.fit_on_texts(np.concatenate([np.array(given_data['merged_headline_shortDescription']), np.array(test_data['merged_headline_shortDescription'])], axis=0))\ngiven_data_merged_sequences=tokenizer.texts_to_sequences(given_data['merged_headline_shortDescription'])\ntest_data_merged_sequences=tokenizer.texts_to_sequences(test_data['merged_headline_shortDescription'])\ngiven_data_merged_seq_padded = pad_sequences(given_data_merged_sequences, maxlen=max_len, padding='post')\ntest_data_merged_seq_padded = pad_sequences(test_data_merged_sequences, maxlen=max_len, padding='post')\nword_index = tokenizer.word_index\nprint('Shape of given data_merged tensor: ' + str(given_data_merged_seq_padded.shape))\nprint('Shape of test data_merged tensor: ' + str(test_data_merged_seq_padded.shape))\nprint('Shape of label tensor: ' + str(labels.shape))\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"861098a90089ba8631f9dc280a8871e50556b15a"},"cell_type":"markdown","source":"**EXTRACTING EMBEDDING MATRIX, TO BE USED AS INITIAL EMBEDDING WEIGHTS IN EMBEDDING LAYER.**"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"c4b4fc75bd251a4ac42779ddec3308e6239425da"},"cell_type":"code","source":"#GLOVE EMBEDDING EXTRACTION\n\n# EMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/glove.840B.300d/glove.840B.300d.txt'\n# def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n# embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n# all_embs = np.stack(embeddings_index.values())\n# emb_mean,emb_std = all_embs.mean(), all_embs.std()\n# embed_size = all_embs.shape[1]\n# del all_embs\n\n# # word_index = tokenizer.word_index\n# nb_words = min(max_words, len(word_index))\n# glove_embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words+1, embed_size))\n# for word, i in word_index.items():\n#     if i >= max_words: continue\n#     embedding_vector = embeddings_index.get(word)\n#     if embedding_vector is not None: glove_embedding_matrix[i] = embedding_vector        \n\n# del embeddings_index\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b6390475947db0994885638941380203e20700c"},"cell_type":"code","source":"# EMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n# def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n# embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n# emb_mean,emb_std = 0.0, 1.0\n# embed_size = 300\n# nb_words = min(max_words, len(word_index))\n# paragram_embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words+1, embed_size))\n# for word, i in word_index.items():\n#     if i >= max_words: continue\n#     embedding_vector = embeddings_index.get(word)\n#     if embedding_vector is not None: paragram_embedding_matrix[i] = embedding_vector   \n        \n# del embeddings_index\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8c1c5bf0715d0d4f7be6dfcb5e08981bb0ddc69"},"cell_type":"markdown","source":"**CONVERTING LABELS INTO ONE-HOT ENCODING FORM**"},{"metadata":{"trusted":true,"_uuid":"77fd2d4399d75f0d2c6b2095919a6abebb3281f4","scrolled":false},"cell_type":"code","source":"# onehot_labels = np.zeros((200853, 41))\n# onehot_labels[np.arange(200853), labels] = 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68f3367bfbbe36f4625c1db275770b17f5b2ac58"},"cell_type":"markdown","source":"**SPLITTING INTO TRAIN AND TEST SETS**"},{"metadata":{"trusted":true,"_uuid":"ad1f85b4787c15b8f48ca256d6f322ce2481dc89"},"cell_type":"code","source":"# #Defining the split into train, validation and test sets.\n# training_samples = 160682\n# validation_samples = 0\n# test_samples = (200853-160682)\n\n# #shuffling\n# indices = np.arange(given_data_merged_seq_padded.shape[0])\n# np.random.shuffle(indices)\n# given_data_merged_seq_padded = given_data_merged_seq_padded[indices]\n# #given_shortDescription_data = given_headline_data[indices]\n# onehot_labels = onehot_labels[indices]\n\n# #x_headline_train = given_headline_data[:training_samples]\n# #x_shortDescription_train = given_shortDescription_data[:training_samples]\n# x_merged_train = given_data_merged_seq_padded[:training_samples]\n# y_train = onehot_labels[:training_samples]\n\n# #x_validation = qns_data[training_samples: training_samples + validation_samples]\n# #y_validation = labels[training_samples: training_samples + validation_samples]\n\n# #x_headline_test = given_headline_data[training_samples + validation_samples: training_samples + validation_samples + test_samples]\n# #x_shortDescription_test = given_shortDescription_data[training_samples + validation_samples: training_samples + validation_samples + test_samples]\n# x_merged_test = given_data_merged_seq_padded[training_samples + validation_samples: training_samples + validation_samples + test_samples]\n# y_test = onehot_labels[training_samples + validation_samples: training_samples + validation_samples + test_samples]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0610bf3f31dcf12c72561098f236f1a7c79231d8"},"cell_type":"markdown","source":"**DEFINATION OF F1 SCORE**"},{"metadata":{"trusted":true,"_uuid":"9b5c7c1515da8951e1542c7307d6e13774df1ea8"},"cell_type":"code","source":"# #Defination of f1 score\n# def f1(y_true, y_pred):\n#     def recall(y_true, y_pred):\n#         \"\"\"Recall metric.\n\n#         Only computes a batch-wise average of recall.\n\n#         Computes the recall, a metric for multi-label classification of\n#         how many relevant items are selected.\n#         \"\"\"\n#         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n#         possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n#         recall = true_positives / (possible_positives + K.epsilon())\n#         return recall\n\n#     def precision(y_true, y_pred):\n#         \"\"\"Precision metric.\n\n#         Only computes a batch-wise average of precision.\n\n#         Computes the precision, a metric for multi-label classification of\n#         how many selected items are relevant.\n#         \"\"\"\n#         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n#         predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n#         precision = true_positives / (predicted_positives + K.epsilon())\n#         return precision\n#     precision = precision(y_true, y_pred)\n#     recall = recall(y_true, y_pred)\n#     return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d850f97eb0c433e89b234f0f043c63402f4f12b"},"cell_type":"markdown","source":"**MODEL ARCHITECTURE(S)**"},{"metadata":{"trusted":true,"_uuid":"88454a7c096c0062593ea205992edbf729c197e9","scrolled":true},"cell_type":"code","source":"# #MODEL 2_glove ARCHITECTURE (BIDIRECTIONAL LSTMs, DROPOUT LAYERS AND A SINGLE RESIDUAL CONNECTION)\n\n# inputTensor_merged = Input(shape = (max_len, ))\n# outputTensor_1 = Embedding(len(word_index)+1, 300, weights = [glove_embedding_matrix], trainable=True)(inputTensor_merged)\n# outputTensor_1=SpatialDropout1D(0.2)(outputTensor_1)\n# outputTensor_2 = Bidirectional(CuDNNLSTM(units=128, return_sequences=True))(outputTensor_1)\n# outputTensor_2 = Bidirectional(CuDNNLSTM(units=128, return_sequences=True))(outputTensor_2)\n# outputTensor_2 = Dropout(0.1)(outputTensor_2)\n# outputTensor_3 = Bidirectional(CuDNNLSTM(units=128, return_sequences=True))(outputTensor_2)\n# outputTensor_3 = Dropout(0.4)(outputTensor_3)\n# outputTensor_3= concatenate([outputTensor_3, outputTensor_1], axis=2)\n# outputTensor_3= Bidirectional(CuDNNLSTM(units=64, return_sequences=False))(outputTensor_3)\n# outputTensor_3 = Dropout(0.4)(outputTensor_3)\n# outputTensor_4 = Dense(units=41, activation='softmax')(outputTensor_3)\n# outputTensor_final = outputTensor_4\n\n# model2_glove = Model(inputTensor_merged, outputTensor_final)\n# model2_glove.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"d3622d7ea636090f1e974a1602637f5f0853c296"},"cell_type":"code","source":"# #MODEL 2_paragram ARCHITECTURE (BIDIRECTIONAL LSTMs, DROPOUT LAYERS AND A SINGLE RESIDUAL CONNECTION)\n\n# inputTensor_merged = Input(shape = (max_len, ))\n# outputTensor_1 = Embedding(len(word_index)+1, 300, weights = [paragram_embedding_matrix], trainable=True)(inputTensor_merged)\n# outputTensor_1=SpatialDropout1D(0.2)(outputTensor_1)\n# outputTensor_2 = Bidirectional(CuDNNLSTM(units=128, return_sequences=True))(outputTensor_1)\n# outputTensor_2 = Bidirectional(CuDNNLSTM(units=128, return_sequences=True))(outputTensor_2)\n# outputTensor_2 = Dropout(0.1)(outputTensor_2)\n# outputTensor_3 = Bidirectional(CuDNNLSTM(units=128, return_sequences=True))(outputTensor_2)\n# outputTensor_3 = Dropout(0.4)(outputTensor_3)\n# outputTensor_3= concatenate([outputTensor_3, outputTensor_1], axis=2)\n# outputTensor_3= Bidirectional(CuDNNLSTM(units=64, return_sequences=False))(outputTensor_3)\n# outputTensor_3 = Dropout(0.4)(outputTensor_3)\n# outputTensor_4 = Dense(units=41, activation='softmax')(outputTensor_3)\n# outputTensor_final = outputTensor_4\n\n# model2_paragram = Model(inputTensor_merged, outputTensor_final)\n# model2_paragram.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"209705b64586f1443cd70b121800d7f37ec88e72","scrolled":true},"cell_type":"code","source":"# #MODEL 4_glove ARCHITECTURE(DEEP LSTM&GRU WITHOUT TRAINING EMBEDDING)\n\n# inputTensor_merged = Input(shape = (max_len, ))\n# outputTensor_1 = Embedding(len(word_index)+1, 300, weights = [glove_embedding_matrix], trainable=False)(inputTensor_merged)\n# outputTensor_1 = SpatialDropout1D(0.1)(outputTensor_1)\n# outputTensor_1 = BatchNormalization()(outputTensor_1)\n# outputTensor_2 = Bidirectional(CuDNNLSTM(units=128, return_sequences=True))(outputTensor_1)\n# outputTensor_3 = Bidirectional(CuDNNLSTM(units=128, return_sequences=True))(outputTensor_2)\n# outputTensor_3 = SpatialDropout1D(0.2)(outputTensor_3)\n# outputTensor_3 = concatenate([outputTensor_2, outputTensor_3, outputTensor_1], axis=2)\n# outputTensor_3 = BatchNormalization()(outputTensor_3)\n# outputTensor_4 = Bidirectional(CuDNNLSTM(units=128, return_sequences=True))(outputTensor_3)\n# outputTensor_4 = SpatialDropout1D(0.3)(outputTensor_4)\n# outputTensor_5 = Bidirectional(CuDNNLSTM(units=64, return_sequences=True))(outputTensor_4)\n# outputTensor_5 = SpatialDropout1D(0.5)(outputTensor_5)\n# outputTensor_5 = concatenate([outputTensor_5, outputTensor_2, outputTensor_4], axis=2)\n# outputTensor_5 = BatchNormalization()(outputTensor_5)\n# outputTensor_6 = Bidirectional(CuDNNLSTM(units=128, return_sequences=True))(outputTensor_5)\n# outputTensor_7 = Bidirectional(CuDNNLSTM(units=64, return_sequences=True))(outputTensor_6)\n# outputTensor_7 = SpatialDropout1D(0.2)(outputTensor_7)\n# outputTensor_8 = Bidirectional(CuDNNLSTM(units=32, return_sequences=False))(outputTensor_7)\n# outputTensor_9 = Dense(units=41, activation='softmax')(outputTensor_8)\n\n# model4_glove = Model(inputTensor_merged, outputTensor_9)\n# model4_glove.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"02792ad02846e08a593db6fc4997d28e2dda056c"},"cell_type":"code","source":"# #MODEL 4_paragram ARCHITECTURE(DEEP LSTM&GRU WITHOUT TRAINING EMBEDDING)\n\n# inputTensor_merged = Input(shape = (max_len, ))\n# outputTensor_1 = Embedding(len(word_index)+1, 300, weights = [glove_embedding_matrix], trainable=False)(inputTensor_merged)\n# outputTensor_1 = SpatialDropout1D(0.1)(outputTensor_1)\n# outputTensor_1 = BatchNormalization()(outputTensor_1)\n# outputTensor_2 = Bidirectional(CuDNNLSTM(units=128, return_sequences=True))(outputTensor_1)\n# outputTensor_3 = Bidirectional(CuDNNLSTM(units=128, return_sequences=True))(outputTensor_2)\n# outputTensor_3 = SpatialDropout1D(0.2)(outputTensor_3)\n# outputTensor_3 = concatenate([outputTensor_2, outputTensor_3, outputTensor_1], axis=2)\n# outputTensor_3 = BatchNormalization()(outputTensor_3)\n# outputTensor_4 = Bidirectional(CuDNNLSTM(units=128, return_sequences=True))(outputTensor_3)\n# outputTensor_4 = SpatialDropout1D(0.3)(outputTensor_4)\n# outputTensor_5 = Bidirectional(CuDNNLSTM(units=64, return_sequences=True))(outputTensor_4)\n# outputTensor_5 = SpatialDropout1D(0.5)(outputTensor_5)\n# outputTensor_5 = concatenate([outputTensor_5, outputTensor_2, outputTensor_4], axis=2)\n# outputTensor_5 = BatchNormalization()(outputTensor_5)\n# outputTensor_6 = Bidirectional(CuDNNLSTM(units=128, return_sequences=True))(outputTensor_5)\n# outputTensor_7 = Bidirectional(CuDNNLSTM(units=64, return_sequences=True))(outputTensor_6)\n# outputTensor_7 = SpatialDropout1D(0.2)(outputTensor_7)\n# outputTensor_8 = Bidirectional(CuDNNLSTM(units=32, return_sequences=False))(outputTensor_7)\n# outputTensor_9 = Dense(units=41, activation='softmax')(outputTensor_8)\n\n# model4_paragram = Model(inputTensor_merged, outputTensor_9)\n# model4_paragram.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8f1ac35e7e140edb91b2fe9f6eff53b79552b9a"},"cell_type":"markdown","source":"**COMPILING AND FITTING CELLS**"},{"metadata":{"trusted":true,"_uuid":"be890da2265fe43dee124a3a96dc9497aecf13bd","scrolled":true},"cell_type":"code","source":"# #compiling and fitting cell for model 2_glove\n# model2_glove.compile(optimizer = 'adam', metrics = ['acc'], loss = 'categorical_crossentropy')\n# gc.collect()\n# model2_glove.fit(x_merged_train, y_train, verbose=2, validation_split=0.1, batch_size=128, epochs=2)\n\n# #compiling and fitting cell for model 2_paragram\n# model2_paragram.compile(optimizer = 'adam', metrics = ['acc'], loss = 'categorical_crossentropy')\n# gc.collect()\n# model2_paragram.fit(x_merged_train, y_train, verbose=2, validation_split=0.1, batch_size=128, epochs=2)\n\n# #compiling and fitting cell for model 4_glove\n# model4_glove.compile(optimizer = 'adam', metrics = ['acc'], loss = 'categorical_crossentropy')\n# gc.collect()\n# model4_glove.fit(x_merged_train, y_train, verbose=2, validation_split=0.1, batch_size=64, epochs=4)\n\n# #compiling and fitting cell for model 4_paragram\n# model4_paragram.compile(optimizer = 'adam', metrics = ['acc'], loss = 'categorical_crossentropy')\n# gc.collect()\n# model4_paragram.fit(x_merged_train, y_train, verbose=2, validation_split=0.1, batch_size=64, epochs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"174a17970be4f14455801de8687dc873d9ae7381","scrolled":true},"cell_type":"code","source":"# #DETERMINATION OF BEST THRESHOLD FOR ROUNDING OFF PREDICTION\n\n# y_predict_x_test=(5*model2_paragram.predict(x_merged_test, verbose=2)+5*model2_glove.predict(x_merged_test, verbose=2)+4*model4_glove.predict(x_merged_test, verbose=2)+4*model4_paragram.predict(x_merged_test, verbose=2))/18\n# threshold=0\n# best_score=0\n# for threshold in np.arange(0, 1, 0.01):\n#     score=f1_score((y_predict_x_test>threshold)*1, y_test, average='weighted')\n#     #print('Threshold: '+ str(threshold) + '  Corresponding F1 score: ' + str(score))\n#     if score > best_score:\n#             best_threshold = threshold\n#             best_score = score\n# print('Best threshold found: '+ str(best_threshold) + ' with a corresponding F1 score of '+ str(best_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27d355a33558c6350716ecdc3d5669258b406497"},"cell_type":"code","source":"# #SAVING MODEL 2 AND MODEL 4 \n\n# #MODEL 2\n# model2_glove.save('Model2_GloVe.h5')\n# model2_paragram.save('Model2_paragram.h5')\n\n# #MODEL 4\n# model4_glove.save('Model4_GloVe.h5')\n# model4_paragram.save('Model4_paragram.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41104024f08798fb33c22f3fb9a46932807286a6"},"cell_type":"code","source":"#IMPORTING TRAINED MODELS FOR PREDICTION ON TEST DATA\nfrom keras.models import load_model\n\nModel2_GloVe = load_model('../input/trainedmodels/Model2_GloVe.h5')\nModel2_paragram = load_model('../input/trainedmodels/Model2_paragram.h5')\nModel4_GloVe = load_model('../input/trainedmodels/Model4_GloVe.h5')\nModel4_paragram = load_model('../input/trainedmodels/Model4_paragram.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92f8ba5314cf24fc183671c11302bba7abfb3b4b"},"cell_type":"code","source":"#PREDICTING ON TEST DATA\n\nx_merged_test_act=test_data_merged_seq_padded\ny_predict_x_test_act=(Model2_paragram.predict(x_merged_test_act, verbose=1)+Model2_GloVe.predict(x_merged_test_act, verbose=1)+Model4_GloVe.predict(x_merged_test_act, verbose=1)+Model4_paragram.predict(x_merged_test_act, verbose=1))/4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4fd363a122537a1cb9d78e2ab0c3ee21f3ebf11a"},"cell_type":"code","source":"y_predict_x_test_act_category_id=np.argmax(y_predict_x_test_act, axis=1)\ny_predict = pd.DataFrame({'category_id': y_predict_x_test_act_category_id})\ny_predict['category'] = y_predict['category_id'].map(id_to_category)\ny_predict['id']=test_data['id']\n\ny_predict=y_predict[['id', 'category']]\n\ny_predict.to_json('submission.json', orient='records')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}